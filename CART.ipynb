{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CART.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKAXNC8uYmSi/1iufIbPn/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhuto1998/Data-Science-Year-2020/blob/master/CART.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9oHEO8M8sNl"
      },
      "source": [
        "# CART: Classification and Regression Trees\r\n",
        "### How to construct a CART for classification?\r\n",
        "Consider a c class problem (c $\\geq$ 2) , with a training set D of size n, containing atleast one observation from each class on a p-variate feature vector $X$. The objective is to generate a binary tree, starting with a root node to which all n training samples are assigned, such that at each non terminal node , the training subset associated with it is split into two on the basis of a feature $X_{j}$ at a value $s$ , where $j$ and $s$ are so chosen that , as a result of the split, the impurity is reduced to the greatest extent. This procedure is repeated till we arrive at terminal nodes whose impurity values are smaller than a pre-specified threshold.\r\n",
        "<br>\r\n",
        "**Impurity**: <br>\r\n",
        "- we need a measure of impurity of a node to help decide on how to split a node or which node to split.\r\n",
        "<br>\r\n",
        "- The measure should be at a maximum when a node is equally divided amongst all classes.\r\n",
        "- the impurity should be 0 if the node is all in one class. \r\n",
        "- For the 3 impurity measures all probabilities are emperical probabilites\r\n",
        "- *information or entropy:* If a node has a proportion $p_{j}$ of each of the classes then the information or entropy is: \r\n",
        "$i_{E}(p) = - \\sum_{j} p_{j}\\log p_{j}$ where 0log0 = 0\r\n",
        "- *Gini's Index*: This is the most used index.\r\n",
        "$i_{G}(p) = \\sum_{i \\neq j}p_{i}p_{j} = 1 - \\sum_{i} p_{i}^{2}$\r\n",
        "- *Misclassification Error:*\r\n",
        "$i_{M}(p) = 1- p_{t} $ where  $t = arg max( p_{k})$\r\n",
        "-When c = 2 then $i_{M}(p) = 1 - max(p,1-p)$ , $i_{G}(p) = 2p(1-p)$ and $i_{E}(p) = -plogp - (1-p)log(1-p)$\r\n",
        "- All the measures are almost similar in behaviour except that: $i_{G} , i_{E}$ are differentiable and more sensitive to changes in node probabilites than $i_{M}$\r\n",
        "- **Tree Impurity:** We define the impurity of a tree to be the sum over all terminal nodes of the impurity of a node multiplied by the proportion of cases that reach that node of the tree.\r\n",
        "- We select the split that most decreases the impurity measure used. This is done over: all possible places for a split and all possible variables to split.\r\n",
        "\r\n",
        "- We keep splitting until the terminal nodes have very cases or are all pure. It was realized that the best approach is to grow a larger tree than required and then to prune it.\r\n",
        "\r\n",
        "Let $i(.)$ be the impurity measure to be used. Any impurity measure has the property that, it attains the maximum value if $p_{i} = \\frac{1}{c} \\forall i = 1(1)c$. It attains minimum value if $p_{i} = 1$ for some $i$.\r\n",
        "<br>\r\n",
        "Let $N$ be an arbitrary non-terminal node that required to be split into two non-empty subsets $N_{L}$ and $N_{R}$. Suppose that, as a consequence of the split , a proportion $P_{L}$ of the training samples at $N$ are allocated to $N_{L}$ and the remaining $P_{R} = (1-P_{L})$ propotion of samples are allocated to $N_{R}$. Then the drop in impurity as a result of this split is: <br>\r\n",
        "$\\Delta i(N) = i(N) - P_{L}i(N_{L}) - (1-P_{L})i(N_{R})$\r\n",
        "\r\n",
        "<br>\r\n",
        "The objective is to define the split in terms of a feature $X_{j}$ and corresponding split value $s$ such that $\\Delta i(N)$ is maximized. Let $D_{N}$ be the subset of $D$ associated with the node $N$. then we can define the training subsets $D_{N_{L}}$ and $D_{N_{R}}$ associated with $N_{L}$ and $N_{R}$ respectively as, <br>\r\n",
        "$D_{N_{L}} = D_{N_{L}}(j,s) = $ { $x|x_{j} \\leq s$ }\r\n",
        "<br>\r\n",
        "$D_{N_{R}} = D_{N_{R}}(j,s) = $ { $x|x_{j} > s$ }\r\n",
        "<br>\r\n",
        "Then we seek $j$ and $s$ such that $\\Delta i(N)$ is maximized w.r.t $j$ and $s$.\r\n",
        "\r\n",
        "## Computational Aspects:\r\n",
        "### Choosing s for a given feature $X_{j} $ at a node m:\r\n",
        "With out loss of generality we assume that the observations $x_{j,1} ,......,x_{j,n_{m}}$ are ordered. A greedy approach is used to optimize $s$. Observe that, for $x_{j,k} \\leq s \\leq x_{j,k+1}$ , the impurity reduction is the same, $\\forall k = 1,2......,n_{m} - 1$. Hence any value of $s$ lying in the range $[x_{j,k},x_{j,k+1})$ can be used interchangeably as far as the impact on impurity measure is concerned. Traditionally, the midpoints of the intervals are taken as possible values of s.\r\n",
        "\r\n",
        "### When to Stop Splitting:\r\n",
        "Too large a tree with leaf nodes having minimal impurities typically leads to overfitting. Too small a tree leads to training set error that is not small enough and consequently performance maybe relatively poor. To take care of these issues, a sufficiently large tree is grown, from which branches are judiciously removed later (pruning) and nodes that were originally non-terminal are made leaf nodes. We will discuss one particular apporach to pruning, namely, cost-complexity pruning, in some detail.\r\n",
        "\r\n",
        "### Assignment of Leaf node labels:\r\n",
        "A leaf node is labeled by that class label that has the largest number of training samples represented at that node.\r\n",
        "\r\n",
        "### Other issues:\r\n",
        "1. **Categorical features:** Consider a qualitative predictor which can take q possible unordered values. Then there are $2^{q-1} - 1$ possible partitions of the q values into two groups, and the computational cost become excessive for large n.\r\n",
        "2. **The loss matrix**: It is possible to incorporate more general loss matrices than the 0-1 loss implicitly assumed here.\r\n",
        "3. **Missing Values:** Specialized methods are available for clearning with these in the context of tree classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN0oAvIQKTku"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}