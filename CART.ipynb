{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CART.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN94ogJ38Sw/YpIrPyqJfSf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhuto1998/Data-Science-Year-2020/blob/master/CART.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9oHEO8M8sNl"
      },
      "source": [
        "# CART: Classification and Regression Trees\r\n",
        "### How to construct a CART for classification?\r\n",
        "Consider a c class problem (c $\\geq$ 2) , with a training set D of size n, containing atleast one observation from each class on a p-variate feature vector $X$. The objective is to generate a binary tree, starting with a root node to which all n training samples are assigned, such that at each non terminal node , the training subset associated with it is split into two on the basis of a feature $X_{j}$ at a value $s$ , where $j$ and $s$ are so chosen that , as a result of the split, the impurity is reduced to the greatest extent. This procedure is repeated till we arrive at terminal nodes whose impurity values are smaller than a pre-specified threshold.\r\n",
        "<br>\r\n",
        "**Impurity**: <br>\r\n",
        "- we need a measure of impurity of a node to help decide on how to split a node or which node to split.\r\n",
        "<br>\r\n",
        "- The measure should be at a maximum when a node is equally divided amongst all classes.\r\n",
        "- the impurity should be 0 if the node is all in one class. \r\n",
        "- *information or entropy:* If a node has a proportion $p_{j}$ of each of the classes then the information or entropy is: \r\n",
        "$i(p) = - \\sum_{j} p_{j}\\log p_{j}$ where 0log0 = 0\r\n",
        "- *Gini's Index*: This is the most used index.\r\n",
        "$i(p) = \\sum_{i \\neq j}p_{i}p_{j} = 1 - \\sum_{i} p_{i}^{2}$\r\n",
        "- **Tree Impurity:** We define the impurity of a tree to be the sum over all terminal nodes of the impurity of a node multiplied by the proportion of cases that reach that node of the tree.\r\n",
        "- We select the split that most decreases the impurity measure used. This is done over: all possible places for a split and all possible variables to split.\r\n",
        "\r\n",
        "- We keep splitting until the terminal nodes have very cases or are all pure. It was realized that the best approach is to grow a larger tree than required and then to prune it.\r\n",
        "\r\n",
        "Let $i(.)$ be the impurity measure to be used. Any impurity measure has the property that, it attains the maximum value if $p_{i} = \\frac{1}{c} \\forall i = 1(1)c$. It attains minimum value if $p_{i} = 1$ for some $i$."
      ]
    }
  ]
}