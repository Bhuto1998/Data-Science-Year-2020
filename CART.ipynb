{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CART.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN2tmKGSUO+5jM5XhdepL44",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhuto1998/Data-Science-Year-2020/blob/master/CART.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9oHEO8M8sNl"
      },
      "source": [
        "# CART: Classification and Regression Trees\r\n",
        "### How to construct a CART for classification?\r\n",
        "Consider a c class problem (c $\\geq$ 2) , with a training set D of size n, containing atleast one observation from each class on a p-variate feature vector $X$. The objective is to generate a binary tree, starting with a root node to which all n training samples are assigned, such that at each non terminal node , the training subset associated with it is split into two on the basis of a feature $X_{j}$ at a value $s$ , where $j$ and $s$ are so chosen that , as a result of the split, the impurity is reduced to the greatest extent. This procedure is repeated till we arrive at terminal nodes whose impurity values are smaller than a pre-specified threshold.\r\n",
        "<br>\r\n",
        "**Impurity**: <br>\r\n",
        "- we need a measure of impurity of a node to help decide on how to split a node or which node to split.\r\n",
        "<br>\r\n",
        "- The measure should be at a maximum when a node is equally divided amongst all classes.\r\n",
        "- the impurity should be 0 if the node is all in one class. \r\n",
        "- For the 3 impurity measures all probabilities are emperical probabilites\r\n",
        "- *information or entropy:* If a node has a proportion $p_{j}$ of each of the classes then the information or entropy is: \r\n",
        "$i_{E}(p) = - \\sum_{j} p_{j}\\log p_{j}$ where 0log0 = 0\r\n",
        "- *Gini's Index*: This is the most used index.\r\n",
        "$i_{G}(p) = \\sum_{i \\neq j}p_{i}p_{j} = 1 - \\sum_{i} p_{i}^{2}$\r\n",
        "- *Misclassification Error:*\r\n",
        "$i_{M}(p) = 1- p_{t} $ where  $t = arg max( p_{k})$\r\n",
        "-When c = 2 then $i_{M}(p) = 1 - max(p,1-p)$ , $i_{G}(p) = 2p(1-p)$ and $i_{E}(p) = -plogp - (1-p)log(1-p)$\r\n",
        "- All the measures are almost similar in behaviour except that: $i_{G} , i_{E}$ are differentiable and more sensitive to changes in node probabilites than $i_{M}$\r\n",
        "- **Tree Impurity:** We define the impurity of a tree to be the sum over all terminal nodes of the impurity of a node multiplied by the proportion of cases that reach that node of the tree.\r\n",
        "- We select the split that most decreases the impurity measure used. This is done over: all possible places for a split and all possible variables to split.\r\n",
        "\r\n",
        "- We keep splitting until the terminal nodes have very cases or are all pure. It was realized that the best approach is to grow a larger tree than required and then to prune it.\r\n",
        "\r\n",
        "Let $i(.)$ be the impurity measure to be used. Any impurity measure has the property that, it attains the maximum value if $p_{i} = \\frac{1}{c} \\forall i = 1(1)c$. It attains minimum value if $p_{i} = 1$ for some $i$.\r\n",
        "<br>\r\n",
        "Let $N$ be an arbitrary non-terminal node that required to be split into two non-empty subsets $N_{L}$ and $N_{R}$. Suppose that, as a consequence of the split , a proportion $P_{L}$ of the training samples at $N$ are allocated to $N_{L}$ and the remaining $P_{R} = (1-P_{L})$ propotion of samples are allocated to $N_{R}$. Then the drop in impurity as a result of this split is: <br>\r\n",
        "$\\Delta i(N) = i(N) - P_{L}i(N_{L}) - (1-P_{L})i(N_{R})$\r\n",
        "\r\n",
        "<br>\r\n",
        "The objective is to define the split in terms of a feature $X_{j}$ and corresponding split value $s$ such that $\\Delta i(N)$ is maximized. Let $D_{N}$ be the subset of $D$ associated with the node $N$. then we can define the training subsets $D_{N_{L}}$ and $D_{N_{R}}$ associated with $N_{L}$ and $N_{R}$ respectively as, <br>\r\n",
        "$D_{N_{L}} = D_{N_{L}}(j,s) = $ { $x|x_{j} \\leq s$ }\r\n",
        "<br>\r\n",
        "$D_{N_{R}} = D_{N_{R}}(j,s) = $ { $x|x_{j} > s$ }\r\n",
        "<br>\r\n",
        "Then we seek $j$ and $s$ such that $\\Delta i(N)$ is maximized w.r.t $j$ and $s$.\r\n",
        "\r\n",
        "## Computational Aspects:\r\n",
        "### Choosing s for a given feature $X_{j} $ at a node m:\r\n",
        "With out loss of generality we assume that the observations $x_{j,1} ,......,x_{j,n_{m}}$ are ordered. A greedy approach is used to optimize $s$. Observe that, for $x_{j,k} \\leq s \\leq x_{j,k+1}$ , the impurity reduction is the same, $\\forall k = 1,2......,n_{m} - 1$. Hence any value of $s$ lying in the range $[x_{j,k},x_{j,k+1})$ can be used interchangeably as far as the impact on impurity measure is concerned. Traditionally, the midpoints of the intervals are taken as possible values of s.\r\n",
        "\r\n",
        "### When to Stop Splitting:\r\n",
        "Too large a tree with leaf nodes having minimal impurities typically leads to overfitting. Too small a tree leads to training set error that is not small enough and consequently performance maybe relatively poor. To take care of these issues, a sufficiently large tree is grown, from which branches are judiciously removed later (pruning) and nodes that were originally non-terminal are made leaf nodes. We will discuss one particular apporach to pruning, namely, cost-complexity pruning, in some detail.\r\n",
        "\r\n",
        "### Assignment of Leaf node labels:\r\n",
        "A leaf node is labeled by that class label that has the largest number of training samples represented at that node.\r\n",
        "\r\n",
        "### Other issues:\r\n",
        "1. **Categorical features:** Consider a qualitative predictor which can take q possible unordered values. Then there are $2^{q-1} - 1$ possible partitions of the q values into two groups, and the computational cost become excessive for large n.\r\n",
        "2. **The loss matrix**: It is possible to incorporate more general loss matrices than the 0-1 loss implicitly assumed here.\r\n",
        "3. **Missing Values:** Specialized methods are available for clearning with these in the context of tree classifiers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsr0eHA0eQoS"
      },
      "source": [
        "# Pruning\r\n",
        "### The Error Rate of a Tree Classifier:\r\n",
        "* Let the true overall misclassification rate for a tree $T$ be $R^{*}(T)$\r\n",
        "* Then the resubstitution estimate for $R^{*}(T)$ is \r\n",
        "$R(T) = \\sum_{t \\in \\bar T} r(t)p(t) = \\sum_{t \\in \\bar T} R(t)$ where $r(t)$ denotes the misclassification error at node t, $p(t)$ is the proportion of training samples reaching node t and $\\bar T$ is the set of all terminal nodes or leaves of T.\r\n",
        "* Furthermore we can show that $R(t) \\geq R(t_{L})+R_{t_{R}} $ where $t_{L}$ and $t_{R}$ are respectively the left and right children of the node $t$\r\n",
        "\r\n",
        "### Initiation:\r\n",
        "* First the tree is grown to a large size denote this maximum size by $T_{max}$\r\n",
        "* There are a few ways of deciding when to stop for example continue until all terminal nodes are pure.\r\n",
        "* As long as the tree is sufficiently large, the size of the initial tree is not critical.\r\n",
        "* The key here is to make the initial tree sufficiently big before pruning.\r\n",
        "\r\n",
        "### Notation:\r\n",
        "* A branch $T_{t}$ of root node $t \\in T$ consists of the node $t$ and all descendandants of $t$ in $T$.\r\n",
        "* Pruning a branch $T_{t}$ from a tree $T$ consists of deleting from $T$ all descendants of $t$. The tree pruned this way will be denoted by $T-T_{t}$\r\n",
        "* If $T'$ is gotten from $T$ by successively pruning off branches , then $T'$ is called a pruned subtree of $T$ and denoted by $T' < T$\r\n",
        "\r\n",
        "### Optimal Subtrees:\r\n",
        "* Even for a moderately sized $T_{max}$, there is an enormously large number of subtrees and an even larger number of ways of pruning the initial tree to get them.\r\n",
        "* It is not possible to perform an exhaustive search.\r\n",
        "* A feasible method of pruning should ensure the following: the subtree is optimal in some sense and the search of the otimal subtree should be computationally tractable.\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "### Minimal Cost Complexity Pruning:\r\n",
        "* $R(T)$ is not a good measure for selecting a subtree because it always favours bigger trees.\r\n",
        "* We need to add a complexity penalty to this resubstitution error rate. The penalty term favours smaller trees, and hence balances with R(t).\r\n",
        "* The Cost-Complexity measure is defined as follows:\r\n",
        "$R_{\\alpha}(T) = R_{T} + \\alpha |\\widetilde T|$ where $|\\widetilde T|$ is the number of terminal nodes in $T$\r\n",
        "* In general, given a preselected $\\alpha$ the objective is to find subtree $T(\\alpha)$ that minimizes $R_{\\alpha}(T)$, i.e., $R_{\\alpha}(T(\\alpha)) = min_{T<T_{max}} R_{\\alpha}(T)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN0oAvIQKTku"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}